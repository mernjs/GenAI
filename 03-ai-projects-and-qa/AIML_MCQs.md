# **AI / ML MCQs (150+ with Answers)**

---

## **SECTION 1: Artificial Intelligence Basics (Q1â€“20)**

**Q1. Artificial Intelligence mainly aims to:**
- A. Replace humans
- B. Mimic human intelligence
- C. Increase hardware speed
- D. Store big data
- **Answer:** B

---

**Q2. Which of the following is NOT AI?**
- A. Image recognition
- B. Rule-based expert system
- C. Database CRUD app
- D. Speech recognition
- **Answer:** C

---

**Q3. AI systems that follow predefined rules are called:**
- A. Learning systems
- B. Reactive machines
- C. Rule-based systems
- D. Neural systems
- **Answer:** C

---

**Q4. Narrow AI refers to:**
- A. Human-level intelligence
- B. One-task intelligence
- C. Super intelligence
- D. Self-aware AI
- **Answer:** B

---

**Q5. Which is an example of General AI?**
- A. Chatbot
- B. Recommendation system
- C. Self-aware robot
- D. Spam filter
- **Answer:** C

---

**Q6. Strong AI means:**
- A. High performance AI
- B. Human-like reasoning
- C. Fast algorithms
- D. Big data processing
- **Answer:** B

---

**Q7. Weak AI is also known as:**
- A. Narrow AI
- B. General AI
- C. Super AI
- D. Reactive AI
- **Answer:** A

---

**Q8. Which field is closest to AI?**
- A. Networking
- B. Operating Systems
- C. Cognitive Science
- D. DBMS
- **Answer:** C

---

**Q9. AI is mainly concerned with:**
- A. Hardware
- B. Intelligence & decision making
- C. Databases
- D. Memory
- **Answer:** B

---

**Q10. Turing Test evaluates:**
- A. Speed
- B. Memory
- C. Intelligence
- D. Accuracy
- **Answer:** C

---

## ðŸ“Š **SECTION 2: Machine Learning Basics (Q21â€“50)**

**Q21. Machine Learning is a subset of:**
- A. AI
- B. Data Science
- C. Statistics
- D. Programming
- **Answer:** A

---

**Q22. ML systems learn from:**
- A. Rules
- B. Instructions
- C. Data
- D. Hardware
- **Answer:** C

---

**Q23. Which is NOT a type of ML?**
- A. Supervised
- B. Unsupervised
- C. Reinforcement
- D. Reactive
- **Answer:** D

---

**Q24. Supervised learning requires:**
- A. Unlabeled data
- B. No data
- C. Labeled data
- D. Random data
- **Answer:** C

---

**Q25. Example of supervised learning:**
- A. Clustering
- B. Dimensionality reduction
- C. Classification
- D. Topic modeling
- **Answer:** C

---

**Q26. Example of unsupervised learning:**
- A. Linear regression
- B. K-Means
- C. Logistic regression
- D. SVM
- **Answer:** B

---

**Q27. Reinforcement learning is based on:**
- A. Labels
- B. Rewards and penalties
- C. Clusters
- D. Distance
- **Answer:** B

---

**Q28. Overfitting means:**
- A. Model performs well on new data
- B. Model memorizes training data
- C. Model is simple
- D. Model has less variance
- **Answer:** B

---

**Q29. Underfitting means:**
- A. Model too complex
- B. Model too simple
- C. Model overtrained
- D. High accuracy
- **Answer:** B

---

**Q30. Bias-variance tradeoff relates to:**
- A. Memory usage
- B. Model complexity
- C. Hardware speed
- D. Data size
- **Answer:** B

---

**Q31. Training dataset is used to:**
- A. Test model
- B. Validate model
- C. Train model
- D. Deploy model
- **Answer:** C

---

**Q32. Test dataset is used to:**
- A. Train model
- B. Tune hyperparameters
- C. Evaluate final model
- D. Store data
- **Answer:** C

---

**Q33. Validation set is used for:**
- A. Final testing
- B. Hyperparameter tuning
- C. Data cleaning
- D. Deployment
- **Answer:** B

---

**Q34. Feature is:**
- A. Output
- B. Input variable
- C. Label
- D. Model
- **Answer:** B

---

**Q35. Label is:**
- A. Input
- B. Output
- C. Feature
- D. Noise
- **Answer:** B

---

**Q36. Accuracy is suitable when:**
- A. Data is imbalanced
- B. Classes are balanced
- C. Cost of errors high
- D. Regression task
- **Answer:** B

---

**Q37. Precision focuses on:**
- A. False positives
- B. False negatives
- C. True negatives
- D. True positives only
- **Answer:** A

---

**Q38. Recall focuses on:**
- A. False positives
- B. False negatives
- C. True positives
- D. True negatives
- **Answer:** B

---

**Q39. F1-score is:**
- A. Average of accuracy
- B. Harmonic mean of precision & recall
- C. Sum of precision & recall
- D. Variance metric
- **Answer:** B

---

**Q40. Confusion matrix is used for:**
- A. Regression
- B. Classification
- C. Clustering
- D. Optimization
- **Answer:** B

---

## ðŸ“ˆ **SECTION 3: ML Algorithms (Q51â€“90)**

**Q51. Linear regression is used for:**
- A. Classification
- B. Regression
- C. Clustering
- D. Ranking
- **Answer:** B

---

**Q52. Logistic regression is used for:**
- A. Regression
- B. Classification
- C. Clustering
- D. Dimensionality reduction
- **Answer:** B

---

**Q53. K-Means is a:**
- A. Supervised algorithm
- B. Unsupervised algorithm
- C. Reinforcement algorithm
- D. Neural algorithm
- **Answer:** B

---

**Q54. K in K-Means represents:**
- A. Kernel
- B. Classes
- C. Clusters
- D. Distance
- **Answer:** C

---

**Q55. Decision trees split data based on:**
- A. Distance
- B. Entropy / Gini
- C. Mean
- D. Variance only
- **Answer:** B

---

**Q56. Random Forest is an ensemble of:**
- A. Neural networks
- B. Linear models
- C. Decision trees
- D. SVMs
- **Answer:** C

---

**Q57. Ensemble learning improves:**
- A. Bias only
- B. Variance only
- C. Accuracy & stability
- D. Speed
- **Answer:** C

---

**Q58. SVM tries to maximize:**
- A. Distance from origin
- B. Margin
- C. Accuracy
- D. Loss
- **Answer:** B

---

**Q59. Kernel trick is used in:**
- A. K-Means
- B. SVM
- C. PCA
- D. KNN
- **Answer:** B

---

**Q60. KNN is:**
- A. Parametric
- B. Non-parametric
- C. Deep model
- D. Ensemble
- **Answer:** B

---

**Q61. KNN prediction depends on:**
- A. Distance
- B. Mean
- C. Variance
- D. Gradient
- **Answer:** A

---

**Q62. PCA is used for:**
- A. Classification
- B. Regression
- C. Dimensionality reduction
- D. Clustering
- **Answer:** C

---

**Q63. PCA is:**
- A. Supervised
- B. Unsupervised
- C. Reinforcement
- D. Semi-supervised
- **Answer:** B

---

**Q64. Gradient Descent is used to:**
- A. Split data
- B. Optimize loss
- C. Reduce data
- D. Label data
- **Answer:** B

---

**Q65. Learning rate controls:**
- A. Model size
- B. Step size in optimization
- C. Number of features
- D. Batch size
- **Answer:** B

---

**Q66. High learning rate may cause:**
- A. Slow convergence
- B. Divergence
- C. Underfitting
- D. Low variance
- **Answer:** B

---

**Q67. Low learning rate causes:**
- A. Fast training
- B. Slow convergence
- C. Overfitting
- D. Crash
- **Answer:** B

---

**Q68. Loss function measures:**
- A. Accuracy
- B. Error
- C. Variance
- D. Bias
- **Answer:** B

---

**Q69. MSE is used for:**
- A. Classification
- B. Regression
- C. Clustering
- D. Ranking
- **Answer:** B

---

**Q70. Cross-entropy is used for:**
- A. Regression
- B. Classification
- C. Clustering
- D. PCA
- **Answer:** B

---

## ðŸ§¬ **SECTION 4: Deep Learning & Neural Networks (Q91â€“120)**

**Q91. Neural network is inspired by:**
- A. Computer architecture
- B. Human brain
- C. Mathematics only
- D. Statistics only
- **Answer:** B

---

**Q92. Basic unit of neural network:**
- A. Layer
- B. Node / Neuron
- C. Weight
- D. Bias
- **Answer:** B

---

**Q93. Activation function adds:**
- A. Linearity
- B. Non-linearity
- C. Speed
- D. Memory
- **Answer:** B

---

**Q94. ReLU stands for:**
- A. Real Linear Unit
- B. Rectified Linear Unit
- C. Recursive Linear Unit
- D. Reduced Linear Unit
- **Answer:** B

---

**Q95. Sigmoid output range:**
- A. âˆ’1 to 1
- B. 0 to 1
- C. 0 to âˆž
- D. âˆ’âˆž to âˆž
- **Answer:** B

---

**Q96. Vanishing gradient affects:**
- A. Input layer
- B. Hidden layers
- C. Output layer
- D. Dataset
- **Answer:** B

---

**Q97. CNN is best for:**
- A. Text
- B. Images
- C. Tabular data
- D. Logs
- **Answer:** B

---

**Q98. RNN is best for:**
- A. Images
- B. Sequential data
- C. Clustering
- D. Regression
- **Answer:** B

---

**Q99. LSTM solves:**
- A. Overfitting
- B. Vanishing gradient
- C. Underfitting
- D. Noise
- **Answer:** B

---

**Q100. Dropout helps prevent:**
- A. Underfitting
- B. Overfitting
- C. Bias
- D. Noise
- **Answer:** B

---

**Q101. Epoch means:**
- A. One data point
- B. One batch
- C. One full dataset pass
- D. One feature
- **Answer:** C

---

**Q102. Batch size refers to:**
- A. Dataset size
- B. Samples per iteration
- C. Features count
- D. Labels count
- **Answer:** B

---

**Q103. Backpropagation is used to:**
- A. Forward pass
- B. Update weights
- C. Predict output
- D. Load data
- **Answer:** B

---

**Q104. Optimizers example:**
- A. ReLU
- B. Adam
- C. Sigmoid
- D. Softmax
- **Answer:** B

---

**Q105. Softmax is used in:**
- A. Binary classification
- B. Multi-class classification
- C. Regression
- D. Clustering
- **Answer:** B

---

## ðŸ§¾ **SECTION 5: NLP, GenAI & LLMs (Q121â€“150)**

**Q121. NLP deals with:**
- A. Images
- B. Audio
- C. Text
- D. Video
- **Answer:** C

---

**Q122. Tokenization means:**
- A. Cleaning text
- B. Splitting text
- C. Translating
- D. Encoding labels
- **Answer:** B

---

**Q123. Stopwords are:**
- A. Important words
- B. Common words
- C. Labels
- D. Tokens
- **Answer:** B

---

**Q124. Stemming reduces words to:**
- A. Root form
- B. Meaningful words
- C. Tokens
- D. Labels
- **Answer:** A

---

**Q125. Lemmatization is:**
- A. Rule-based
- B. Dictionary-based
- C. Random
- D. Statistical
- **Answer:** B

---

**Q126. Embeddings represent:**
- A. Words as text
- B. Words as vectors
- C. Sentences as labels
- D. Numbers as text
- **Answer:** B

---

**Q127. Word2Vec is used for:**
- A. Translation
- B. Embeddings
- C. Classification
- D. OCR
- **Answer:** B

---

**Q128. Transformer models use:**
- A. RNN
- B. CNN
- C. Attention
- D. KNN
- **Answer:** C

---

**Q129. Attention helps model to:**
- A. Memorize
- B. Focus on relevant tokens
- C. Reduce size
- D. Speed training
- **Answer:** B

---

**Q130. LLM stands for:**
- A. Long Learning Model
- B. Large Language Model
- C. Logical Learning Machine
- D. Linear Language Model
- **Answer:** B

---

**Q131. LLMs are trained on:**
- A. Small datasets
- B. Rules
- C. Large text corpora
- D. Images only
- **Answer:** C

---

**Q132. Fine-tuning means:**
- A. Training from scratch
- B. Adjusting pre-trained model
- C. Cleaning data
- D. Reducing features
- **Answer:** B

---

**Q133. RAG stands for:**
- A. Random AI Generator
- B. Retrieval-Augmented Generation
- C. Recursive AI Graph
- D. Real-time AI Generator
- **Answer:** B

---

**Q134. RAG improves:**
- A. Model size
- B. Factual accuracy
- C. Training speed
- D. GPU usage
- **Answer:** B

---

**Q135. Hallucination in LLMs means:**
- A. Model crash
- B. Incorrect confident answers
- C. Slow response
- D. Overfitting
- **Answer:** B

---

**Q136. Prompt engineering focuses on:**
- A. Model weights
- B. Input design
- C. GPU tuning
- D. Dataset size
- **Answer:** B

---

**Q137. Zero-shot learning means:**
- A. No training
- B. No examples in prompt
- C. No labels
- D. No data
- **Answer:** B

---

**Q138. Few-shot learning provides:**
- A. Many examples
- B. Few examples
- C. No examples
- D. All data
- **Answer:** B

---

**Q139. Temperature controls:**
- A. Speed
- B. Randomness
- C. Accuracy
- D. Tokens
- **Answer:** B

---

**Q140. High temperature gives:**
- A. Deterministic output
- B. Creative output
- C. Same output
- D. No output
- **Answer:** B

---

**Q141. Low temperature gives:**
- A. Random output
- B. Deterministic output
- C. Long output
- D. No output
- **Answer:** B

---

**Q142. Token limit affects:**
- A. Training
- B. Context size
- C. GPU speed
- D. Accuracy only
- **Answer:** B

---

**Q143. Context window is:**
- A. Model size
- B. Max tokens model sees
- C. Training time
- D. Dataset
- **Answer:** B

---

**Q144. Vector database stores:**
- A. Text
- B. Images
- C. Embeddings
- D. Models
- **Answer:** C

---

**Q145. Similarity search uses:**
- A. Sorting
- B. Distance metrics
- C. Hashing
- D. Indexing only
- **Answer:** B

---

**Q146. Cosine similarity measures:**
- A. Distance
- B. Angle between vectors
- C. Length
- D. Magnitude
- **Answer:** B

---

**Q147. LLM evaluation focuses on:**
- A. Accuracy only
- B. Factuality, relevance, safety
- C. Speed only
- D. Memory only
- **Answer:** B

---

**Q148. Guardrails help prevent:**
- A. Overfitting
- B. Unsafe outputs
- C. Slow inference
- D. Training failure
- **Answer:** B

---

**Q149. AI agents can:**
- A. Only respond
- B. Plan, decide, act
- C. Store data only
- D. Train models only
- **Answer:** B

---

**Q150. Agentic AI means:**
- A. Rule-based AI
- B. Autonomous decision making
- C. Supervised learning
- D. Static models
- **Answer:** B

---

# ðŸ¤– **AI / ML MCQs (200+ Questions with Answers)**

---

## ðŸ§  **SECTION 1: AI Fundamentals (Q1â€“25)**

**Q1. AI mainly focuses on:**
- A. Storage
- B. Computation
- C. Intelligent behavior
- D. Networking
- **Answer** C

**Q2. Which AI type can learn from experience?**
- A. Reactive
- B. Limited memory
- C. Theory of mind
- D. Self-aware
- **Answer** B

**Q3. Expert systems are based on:**
- A. Neural networks
- B. Rules and knowledge base
- C. Reinforcement learning
- D. Deep learning
- **Answer** B

**Q4. Which is NOT an AI task?**
- A. Speech recognition
- B. Image classification
- C. Sorting numbers
- D. Game playing
- **Answer** C

**Q5. Knowledge representation deals with:**
- A. Data storage
- B. How AI stores facts
- C. Model training
- D. Hardware
- **Answer** B

**Q6. Heuristic search uses:**
- A. Random guesses
- B. Domain knowledge
- C. Labels
- D. Gradients
- **Answer** B

**Q7. Turing Test evaluates:**
- A. Accuracy
- B. Intelligence
- C. Speed
- D. Memory
- **Answer** B

**Q8. Weak AI is also called:**
- A. General AI
- B. Narrow AI
- C. Super AI
- D. Human AI
- **Answer** B

**Q9. Strong AI refers to:**
- A. High compute
- B. Human-like intelligence
- C. Fast models
- D. Deep learning
- **Answer** B

**Q10. AI agents perceive environment using:**
- A. Sensors
- B. Actuators
- C. Memory
- D. CPU
- **Answer** A

**Q11. AI agents act using:**
- A. Sensors
- B. Actuators
- C. Memory
- D. Storage
- **Answer** B

**Q12. Goal-based agents focus on:**
- A. Current state
- B. End result
- C. Random actions
- D. Rules only
- **Answer** B

**Q13. Utility-based agents choose actions based on:**
- A. Cost
- B. Utility value
- C. Accuracy
- D. Speed
- **Answer** B

**Q14. Search space refers to:**
- A. Dataset
- B. All possible states
- C. Memory
- D. Parameters
- **Answer** B

**Q15. BFS uses:**
- A. Stack
- B. Queue
- C. Priority queue
- D. Tree
- **Answer** B

**Q16. DFS uses:**
- A. Queue
- B. Stack
- C. Heap
- D. Graph
- **Answer** B

**Q17. A* search uses:**
- A. Random search
- B. Heuristic + cost
- C. DFS
- D. BFS
- **Answer** B

**Q18. AI planning focuses on:**
- A. Data
- B. Sequence of actions
- C. Labels
- D. Clusters
- **Answer** B

**Q19. State space explosion means:**
- A. Few states
- B. Too many states
- C. No states
- D. Error
- **Answer** B

**Q20. AI reasoning includes:**
- A. Deduction
- B. Induction
- C. Abduction
- D. All
- **Answer** D

**Q21. Knowledge base contains:**
- A. Facts
- B. Rules
- C. Both
- D. None
- **Answer** C

**Q22. Inference engine does:**
- A. Stores data
- B. Applies rules
- C. Trains model
- D. Optimizes loss
- **Answer** B

**Q23. Planning problems involve:**
- A. Initial state
- B. Goal state
- C. Actions
- D. All
- **Answer** D

**Q24. Environment with uncertainty is:**
- A. Deterministic
- B. Stochastic
- C. Static
- D. Observable
- **Answer** B

**Q25. Fully observable environment means:**
- A. Partial info
- B. Complete info
- C. No sensors
- D. Random
- **Answer** B

---

## ðŸ“Š **SECTION 2: Machine Learning Basics (Q26â€“60)**

**Q26. ML is a subset of:**
- A. AI
- B. Data science
- C. Programming
- D. Statistics
- **Answer** A

**Q27. ML learns patterns from:**
- A. Rules
- B. Code
- C. Data
- D. Hardware
- **Answer** C

**Q28. Supervised learning uses:**
- A. Unlabeled data
- B. Labeled data
- C. No data
- D. Random data
- **Answer** B

**Q29. Unsupervised learning uses:**
- A. Labeled data
- B. Unlabeled data
- C. Rewards
- D. Rules
- **Answer** B

**Q30. Reinforcement learning uses:**
- A. Labels
- B. Rewards
- C. Clusters
- D. Features
- **Answer** B

**Q31. Classification predicts:**
- A. Continuous values
- B. Categories
- C. Clusters
- D. Distances
- **Answer** B

**Q32. Regression predicts:**
- A. Labels
- B. Categories
- C. Continuous values
- D. Classes
- **Answer** C

**Q33. Clustering groups data based on:**
- A. Labels
- B. Similarity
- C. Targets
- D. Rules
- **Answer** B

**Q34. Feature engineering improves:**
- A. Hardware
- B. Data quality
- C. Model performance
- D. GPU
- **Answer** C

**Q35. Overfitting means:**
- A. Good generalization
- B. Memorizing training data
- C. Simple model
- D. Low variance
- **Answer** B

**Q36. Underfitting means:**
- A. Too complex
- B. Too simple
- C. High accuracy
- D. Overtrained
- **Answer** B

**Q37. Bias refers to:**
- A. Error from simplicity
- B. Error from noise
- C. Random error
- D. Variance only
- **Answer** A

**Q38. Variance refers to:**
- A. Error from simplicity
- B. Error from sensitivity
- C. Label noise
- D. Bias
- **Answer** B

**Q39. Biasâ€“variance tradeoff balances:**
- A. Speed & memory
- B. Simplicity & complexity
- C. Accuracy & recall
- D. Cost & time
- **Answer** B

**Q40. Training data is used to:**
- A. Evaluate model
- B. Tune hyperparameters
- C. Learn patterns
- D. Deploy
- **Answer** C

**Q41. Validation data is used for:**
- A. Final testing
- B. Hyperparameter tuning
- C. Training
- D. Storage
- **Answer** B

**Q42. Test data is used for:**
- A. Training
- B. Tuning
- C. Final evaluation
- D. Cleaning
- **Answer** C

**Q43. Data leakage causes:**
- A. Better accuracy
- B. Unrealistic performance
- C. Underfitting
- D. Noise
- **Answer** B

**Q44. Accuracy is misleading when:**
- A. Balanced data
- B. Imbalanced data
- C. Small data
- D. Clean data
- **Answer** B

**Q45. Precision measures:**
- A. Missed positives
- B. False positives
- C. True negatives
- D. False negatives
- **Answer** B

**Q46. Recall measures:**
- A. Missed positives
- B. False positives
- C. True negatives
- D. Accuracy
- **Answer** A

**Q47. F1-score balances:**
- A. Accuracy & loss
- B. Precision & recall
- C. Bias & variance
- D. Speed & cost
- **Answer** B

**Q48. ROC curve plots:**
- A. Precision vs recall
- B. TPR vs FPR
- C. Loss vs epochs
- D. Accuracy vs time
- **Answer** B

**Q49. AUC value close to 1 means:**
- A. Bad model
- B. Random model
- C. Good model
- D. Overfitting
- **Answer** C

**Q50. Confusion matrix is used in:**
- A. Regression
- B. Classification
- C. Clustering
- D. PCA
- **Answer** B

---

## ðŸ“ˆ **SECTION 3: ML Algorithms (Q61â€“110)**

**Q61. Linear regression minimizes:**
- A. MAE
- B. MSE
- C. Accuracy
- D. Entropy
- **Answer** B

**Q62. Logistic regression outputs:**
- A. Any value
- B. Probability
- C. Cluster
- D. Distance
- **Answer** B

**Q63. Logistic regression uses:**
- A. Sigmoid
- B. ReLU
- C. Softmax
- D. Tanh
- **Answer** A

**Q64. Decision tree splits use:**
- A. Distance
- B. Entropy/Gini
- C. Gradient
- D. Mean
- **Answer** B

**Q65. Decision trees tend to:**
- A. Underfit
- B. Overfit
- C. Ignore noise
- D. Be linear
- **Answer** B

**Q66. Pruning helps to:**
- A. Increase bias
- B. Reduce overfitting
- C. Increase depth
- D. Reduce data
- **Answer** B

**Q67. Random Forest is:**
- A. Single tree
- B. Ensemble of trees
- C. Neural network
- D. Linear model
- **Answer** B

**Q68. Bagging reduces:**
- A. Bias
- B. Variance
- C. Both
- D. None
- **Answer** B

**Q69. Boosting focuses on:**
- A. Random data
- B. Weak learners
- C. Clustering
- D. Noise
- **Answer** B

**Q70. AdaBoost increases weight of:**
- A. Correct samples
- B. Misclassified samples
- C. All samples
- D. None
- **Answer** B

**Q71. XGBoost is known for:**
- A. Simplicity
- B. Performance
- C. Interpretability
- D. Slow training
- **Answer** B

**Q72. KNN stores:**
- A. Model parameters
- B. Entire dataset
- C. Gradients
- D. Rules
- **Answer** B

**Q73. KNN is sensitive to:**
- A. Feature scaling
- B. Initialization
- C. Learning rate
- D. Epochs
- **Answer** A

**Q74. Distance metric in KNN:**
- A. Accuracy
- B. Euclidean
- C. Entropy
- D. Gini
- **Answer** B

**Q75. SVM maximizes:**
- A. Accuracy
- B. Margin
- C. Probability
- D. Loss
- **Answer** B

**Q76. Kernel trick helps in:**
- A. Linear separation
- B. Non-linear separation
- C. Dimensionality reduction
- D. Clustering
- **Answer** B

**Q77. Common SVM kernel:**
- A. ReLU
- B. Sigmoid
- C. RBF
- D. Softmax
- **Answer** C

**Q78. Naive Bayes is based on:**
- A. Distance
- B. Probability
- C. Gradient
- D. Entropy only
- **Answer** B

**Q79. Naive Bayes assumes:**
- A. Feature independence
- B. Feature dependence
- C. Linear relation
- D. No noise
- **Answer** A

**Q80. K-Means requires:**
- A. Labels
- B. K value
- C. Distance labels
- D. Output variable
- **Answer** B

**Q81. K-Means converges to:**
- A. Global optimum
- B. Local optimum
- C. Random
- D. Error
- **Answer** B

**Q82. Elbow method helps choose:**
- A. Features
- B. K
- C. Learning rate
- D. Epochs
- **Answer** B

**Q83. PCA reduces:**
- A. Samples
- B. Features
- C. Labels
- D. Noise only
- **Answer** B

**Q84. PCA components are:**
- A. Orthogonal
- B. Parallel
- C. Random
- D. Identical
- **Answer** A

**Q85. PCA maximizes:**
- A. Mean
- B. Variance
- C. Accuracy
- D. Distance
- **Answer** B

**Q86. PCA is sensitive to:**
- A. Scaling
- B. Labels
- C. Noise only
- D. Bias
- **Answer** A

**Q87. t-SNE is mainly used for:**
- A. Training
- B. Visualization
- C. Prediction
- D. Optimization
- **Answer** B

**Q88. Gradient Descent updates:**
- A. Features
- B. Weights
- C. Labels
- D. Data
- **Answer** B

**Q89. Batch GD uses:**
- A. One sample
- B. Entire dataset
- C. Mini batch
- D. Random
- **Answer** B

**Q90. SGD uses:**
- A. Full dataset
- B. One sample
- C. Entire batch
- D. No data
- **Answer** B

**Q91. Mini-batch GD uses:**
- A. One sample
- B. Small batch
- C. Entire dataset
- D. None
- **Answer** B

**Q92. Learning rate too high causes:**
- A. Slow training
- B. Divergence
- C. Underfitting
- D. Low variance
- **Answer** B

**Q93. Learning rate too low causes:**
- A. Fast convergence
- B. Slow convergence
- C. Overfitting
- D. Crash
- **Answer** B

**Q94. Regularization reduces:**
- A. Bias
- B. Variance
- C. Accuracy
- D. Data
- **Answer** B

**Q95. L1 regularization promotes:**
- A. Small weights
- B. Sparse weights
- C. Large weights
- D. Dense model
- **Answer** B

**Q96. L2 regularization penalizes:**
- A. Absolute weights
- B. Squared weights
- C. Gradients
- D. Bias
- **Answer** B

**Q97. Feature scaling improves:**
- A. Accuracy only
- B. Gradient descent
- C. Labels
- D. Noise
- **Answer** B

**Q98. Standardization means:**
- A. Minâ€“max
- B. Mean 0, std 1
- C. 0â€“1 range
- D. Clipping
- **Answer** B

**Q99. Normalization means:**
- A. Mean 0
- B. Std 1
- C. Scale to range
- D. Encode labels
- **Answer** C

**Q100. Cross-validation improves:**
- A. Bias
- B. Reliability
- C. Speed
- D. Data size
- **Answer** B

---

## ðŸ§¬ **SECTION 4: Deep Learning (Q111â€“160)**

**Q111. Neural networks are inspired by:**
- A. Statistics
- B. Biology
- C. Physics
- D. Chemistry
- **Answer** B

**Q112. Neuron output is:**
- A. Weighted sum + activation
- B. Mean
- C. Distance
- D. Label
- **Answer** A

**Q113. Activation functions add:**
- A. Linearity
- B. Non-linearity
- C. Noise
- D. Speed
- **Answer** B

**Q114. ReLU outputs:**
- A. Negative values
- B. Zero for negatives
- C. Between 0â€“1
- D. âˆ’1 to 1
- **Answer** B

**Q115. Sigmoid problem:**
- A. Overfitting
- B. Vanishing gradient
- C. Noise
- D. Bias
- **Answer** B

**Q116. Tanh output range:**
- A. 0â€“1
- B. âˆ’1 to 1
- C. 0â€“âˆž
- D. âˆ’âˆž to âˆž
- **Answer** B

**Q117. CNNs are best for:**
- A. Time series
- B. Images
- C. Text only
- D. Tabular data
- **Answer** B

**Q118. CNN uses:**
- A. Fully connected layers only
- B. Convolution
- C. Recurrence
- D. Trees
- **Answer** B

**Q119. Pooling reduces:**
- A. Channels
- B. Spatial size
- C. Classes
- D. Loss
- **Answer** B

**Q120. RNN handles:**
- A. Independent data
- B. Sequential data
- C. Images
- D. Graphs
- **Answer** B

**Q121. RNN problem:**
- A. Overfitting
- B. Vanishing gradient
- C. Bias
- D. Noise
- **Answer** B

**Q122. LSTM solves:**
- A. Overfitting
- B. Vanishing gradient
- C. Bias
- D. Noise
- **Answer** B

**Q123. GRU is:**
- A. More complex than LSTM
- B. Simpler than LSTM
- C. Same
- D. Slower
- **Answer** B

**Q124. Dropout randomly:**
- A. Adds neurons
- B. Removes neurons
- C. Freezes neurons
- D. Copies neurons
- **Answer** B

**Q125. Dropout reduces:**
- A. Bias
- B. Overfitting
- C. Underfitting
- D. Noise
- **Answer** B

**Q126. Epoch means:**
- A. One batch
- B. One full pass
- C. One sample
- D. One layer
- **Answer** B

**Q127. Batch size affects:**
- A. Memory usage
- B. Training speed
- C. Convergence
- D. All
- **Answer** D

**Q128. Backpropagation computes:**
- A. Loss
- B. Gradients
- C. Predictions
- D. Labels
- **Answer** B

**Q129. Optimizer example:**
- A. ReLU
- B. Adam
- C. Sigmoid
- D. Softmax
- **Answer** B

**Q130. Adam combines:**
- A. SGD + RMSProp
- B. GD + momentum
- C. RMS + Ada
- D. None
- **Answer** A

**Q131. Weight initialization matters because:**
- A. Speed
- B. Convergence
- C. Memory
- D. Labels
- **Answer** B

**Q132. Exploding gradients cause:**
- A. Zero updates
- B. Very large updates
- C. Underfitting
- D. Noise
- **Answer** B

**Q133. Batch normalization helps with:**
- A. Scaling
- B. Faster convergence
- C. Stability
- D. All
- **Answer** D

**Q134. Loss vs epoch plot helps detect:**
- A. Data leakage
- B. Overfitting
- C. Bias
- D. Noise
- **Answer** B

**Q135. Early stopping prevents:**
- A. Underfitting
- B. Overfitting
- C. Bias
- D. Variance
- **Answer** B

---

## ðŸ§¾ **SECTION 5: NLP, GenAI, LLMs (Q161â€“210)**

**Q161. NLP deals with:**
- A. Images
- B. Text
- C. Audio
- D. Video
- **Answer** B

**Q162. Tokenization splits text into:**
- A. Characters only
- B. Tokens
- C. Sentences only
- D. Labels
- **Answer** B

**Q163. Stopwords are removed to:**
- A. Increase noise
- B. Reduce noise
- C. Increase length
- D. Add meaning
- **Answer** B

**Q164. Bag-of-Words ignores:**
- A. Frequency
- B. Order
- C. Vocabulary
- D. Tokens
- **Answer** B

**Q165. TF-IDF reduces weight of:**
- A. Rare words
- B. Frequent words
- C. Important words
- D. Labels
- **Answer** B

**Q166. Word embeddings capture:**
- A. Syntax only
- B. Semantics
- C. Labels
- D. Grammar rules
- **Answer** B

**Q167. Word2Vec models:**
- A. Images
- B. Words
- C. Audio
- D. Video
- **Answer** B

**Q168. Transformer architecture uses:**
- A. RNN
- B. CNN
- C. Attention
- D. Trees
- **Answer** C

**Q169. Self-attention helps model:**
- A. Ignore context
- B. Focus on relevant tokens
- C. Reduce size
- D. Speed training
- **Answer** B

**Q170. LLM stands for:**
- A. Large Language Model
- B. Long Learning Machine
- C. Linear Language Model
- D. Logical LM
- **Answer** A

**Q171. LLMs are trained on:**
- A. Small data
- B. Large text corpora
- C. Rules
- D. Labels only
- **Answer** B

**Q172. Fine-tuning adapts model to:**
- A. New hardware
- B. Specific task
- C. Larger dataset
- D. New optimizer
- **Answer** B

**Q173. RAG combines:**
- A. Training + inference
- B. Retrieval + generation
- C. CNN + RNN
- D. Prompt + response
- **Answer** B

**Q174. RAG reduces:**
- A. Model size
- B. Hallucinations
- C. Cost only
- D. Latency only
- **Answer** B

**Q175. Hallucination means:**
- A. Model crash
- B. Confident wrong output
- C. Slow output
- D. Overfitting
- **Answer** B

**Q176. Prompt engineering improves:**
- A. Model weights
- B. Output quality
- C. Training speed
- D. GPU usage
- **Answer** B

**Q177. Zero-shot learning uses:**
- A. No training
- B. No examples
- C. Full dataset
- D. Labels
- **Answer** B

**Q178. Few-shot learning provides:**
- A. Many examples
- B. Few examples
- C. No examples
- D. All data
- **Answer** B

**Q179. Temperature controls:**
- A. Speed
- B. Randomness
- C. Tokens
- D. Memory
- **Answer** B

**Q180. Low temperature gives:**
- A. Creative output
- B. Deterministic output
- C. Random output
- D. Long output
- **Answer** B

**Q181. Context window is:**
- A. Training size
- B. Max tokens seen
- C. Output size
- D. Dataset
- **Answer** B

**Q182. Vector DB stores:**
- A. Text
- B. Images
- C. Embeddings
- D. Models
- **Answer** C

**Q183. Similarity search uses:**
- A. Sorting
- B. Distance metrics
- C. Hashing only
- D. Rules
- **Answer** B

**Q184. Cosine similarity measures:**
- A. Distance
- B. Angle
- C. Magnitude
- D. Length
- **Answer** B

**Q185. AI agents can:**
- A. Only chat
- B. Plan and act
- C. Only retrieve
- D. Only classify
- **Answer** B

**Q186. Agentic AI means:**
- A. Static responses
- B. Autonomous decisions
- C. Rule-based AI
- D. Weak AI
- **Answer** B

**Q187. Tool calling allows LLM to:**
- A. Train itself
- B. Use external tools
- C. Change weights
- D. Increase tokens
- **Answer** B

**Q188. Guardrails are used for:**
- A. Speed
- B. Safety
- C. Accuracy only
- D. Memory
- **Answer** B

**Q189. Evaluation metrics for LLMs include:**
- A. Accuracy only
- B. BLEU, ROUGE
- C. Loss only
- D. F1 only
- **Answer** B

**Q190. LLM inference means:**
- A. Training
- B. Prediction
- C. Fine-tuning
- D. Labeling
- **Answer** B